{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoEiQ-aBnQLt"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PZ3hSrj4nQLu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(directory):\n",
    "    \n",
    "    file_paths = []  # List which will store all of the full filepaths.\n",
    "    file_image = []  # Image filename\n",
    "   \n",
    "    # Walk the tree.\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Join the two strings in order to form the full filepath.\n",
    "                       \n",
    "            filepath = os.path.join(root, filename)\n",
    "            \n",
    "            file_paths.append(filepath)  # Add it to the list.       \n",
    "            file_image.append(filename.split('.')[0])\n",
    "\n",
    "    return file_paths, file_image  # Self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "        \n",
    "    def __init__(self, df_X, arr_Y, batch_size=32,shuffle=False,image_size=128):\n",
    "        self.batch_size = batch_size\n",
    "        self.df_X = df_X\n",
    "        self.arr_Y = arr_Y\n",
    "        self.indices = self.df_X.index.tolist()\n",
    "        self.shuffle = shuffle\n",
    "        self.image_size = image_size\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.indices[k] for k in index]\n",
    "        \n",
    "        X, y = self.__get_data(batch)\n",
    "        return X, y\n",
    "    \n",
    "    def n(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X = []\n",
    "        y = []\n",
    "               \n",
    "        for i, id in enumerate(batch):\n",
    "                       \n",
    "            # Data\n",
    "            file = self.df_X.iloc[self.indices[id],0]\n",
    "            img = load_img(file,color_mode='rgb', target_size=(self.image_size,self.image_size),interpolation='nearest')\n",
    "                                               \n",
    "            img = img_to_array(img).astype(np.float32)\n",
    "                        \n",
    "            X.append(img)\n",
    "            y.append(self.arr_Y[self.indices[id]])\n",
    "            \n",
    "        return np.array(X), np.array(y).reshape(self.batch_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsRecorder:\n",
    "    def __init__(self, data=None):\n",
    "        \"\"\"\n",
    "        data: ndarray, shape (nobservations, ndimensions)\n",
    "        \"\"\"\n",
    "        if data is not None:\n",
    "            data = np.atleast_2d(data)\n",
    "            self.mean = data.mean()\n",
    "            self.std  = data.std()\n",
    "            self.nobservations = data.shape[0]\n",
    "            self.ndimensions   = data.shape[1]\n",
    "        else:\n",
    "            self.nobservations = 0\n",
    "\n",
    "    def update(self, data):\n",
    "        \"\"\"\n",
    "        data: ndarray, shape (nobservations, ndimensions)\n",
    "        \"\"\"\n",
    "        if self.nobservations == 0:\n",
    "            self.__init__(data)\n",
    "        else:\n",
    "            data = np.atleast_2d(data)\n",
    "            if data.shape[1] != self.ndimensions:\n",
    "                raise ValueError(\"Data dims don't match prev observations.\")\n",
    "\n",
    "            newmean = data.mean()\n",
    "            newstd  = data.std()\n",
    "\n",
    "            m = self.nobservations * 1.0\n",
    "            n = data.shape[0]\n",
    "\n",
    "            tmp = self.mean\n",
    "\n",
    "            self.mean = m/(m+n)*tmp + n/(m+n)*newmean\n",
    "            self.std  = m/(m+n)*self.std**2 + n/(m+n)*newstd**2 +\\\n",
    "                        m*n/(m+n)**2 * (tmp - newmean)**2\n",
    "            self.std  = np.sqrt(self.std)\n",
    "\n",
    "            self.nobservations += n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rk_OOT73nQLv"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "#data = data.loc[data.bolts <= 15]\n",
    "#data = data.loc[data.bolts > 0]\n",
    "#data.reset_index(drop=True,inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1EBdEeEnQLv"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ttm-h1-pnQLw"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(df_X=data, arr_Y=data.bolts.values, batch_size=batch_size, shuffle=True,image_size=image_size)\n",
    "STEP_SIZE_TRAIN = train_generator.n()//train_generator.batch_size\n",
    "\n",
    "mystats = StatsRecorder()\n",
    "\n",
    "for image in train_generator:\n",
    "    \n",
    "    data = image[0].reshape((batch_size,image_size*image_size*3))\n",
    "    \n",
    "    mystats.update(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136.78040649936997\n",
      "107.97868633912864\n"
     ]
    }
   ],
   "source": [
    "print(mystats.mean)\n",
    "print(mystats.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
